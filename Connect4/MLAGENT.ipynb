{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Connect4 import Connect4\n",
    "from Robots import Robots\n",
    "from GamePlay import GamePlay\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class that creates the connect4 environment\n",
    "class DeepQLAgent():\n",
    "\n",
    "    def __init__(self,player=1,gameplay=GamePlay(),memory=None):\n",
    "        self.player=player\n",
    "        \n",
    "        self.gameplay=gameplay\n",
    "        self.ROWS = self.gameplay.ROWS\n",
    "        self.COLUMNS = self.gameplay.COLUMNS\n",
    "        \n",
    "        self.batch_size = 100\n",
    "        self.lr=.01\n",
    "        self.INPUT_SHAPE=(self.ROWS, self.COLUMNS, 1)\n",
    "        self.action_size =self.COLUMNS\n",
    "        self.gamma=.9\n",
    "        self.epsilon = 1.0 #starting explore probability\n",
    "        self.explore_stop = .01 #stopping explore rate\n",
    "        self.decay_rate = .000001 #decay rate # 0.00001\n",
    "\n",
    "        self.memory_size = 1000 # number of experiences to keep\n",
    "        self.memory_start = 100 # starting size of memory bank\n",
    "        self.memory=self._memory_initiate()\n",
    "        \n",
    "        self.model=self._build_model()\n",
    "\n",
    "    #build model\n",
    "    def _build_model(self):\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=self.INPUT_SHAPE))\n",
    "\n",
    "        #model.add(Conv2D(64, (3, 3), activation = 'relu')) \n",
    "        model.add(layers.MaxPooling2D(pool_size = (2, 2))) \n",
    "        model.add(layers.Dropout(0.25)) \n",
    "        model.add(layers.Flatten()) \n",
    "        model.add(layers.Dense(64, activation = 'relu')) \n",
    "        model.add(layers.Dense(32,activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size,activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                     loss='mse',\n",
    "                     metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "\n",
    "    def _memory_initiate(self):\n",
    "        memory=[]\n",
    "        turn = 0\n",
    "        #player=1\n",
    "        status = None\n",
    "        while len(memory)<self.memory_start:\n",
    "            if status != 'Keep Playing!': #if we are in a terminal state restart game\n",
    "                self.gameplay.reset()\n",
    "                turn = 0\n",
    "\n",
    "            state = self.gameplay.BOARD.copy()\n",
    "            actions = self.gameplay.Get_Legal_Moves(self.gameplay.BOARD)\n",
    "            action = np.random.choice(actions,1)[0]\n",
    "            #take random actions. record their states,actions, rewards, next states, and status\n",
    "\n",
    "            #player1\n",
    "            if turn % 2 ==0:\n",
    "                self.gameplay.Add_Piece(1,action,self.gameplay.BOARD)\n",
    "                #turn +=1\n",
    "\n",
    "            #player2\n",
    "            elif turn % 2 ==1:\n",
    "                self.gameplay.Add_Piece(2,action,self.gameplay.BOARD)\n",
    "                #turn +=1\n",
    "\n",
    "\n",
    "            status = self.gameplay.Check_Goal(self.gameplay.BOARD)\n",
    "            if status !='Keep Playing!':\n",
    "                state_prime = np.zeros((self.gameplay.ROWS,self.gameplay.COLUMNS))\n",
    "                state_prime[state_prime < 1] = self.player\n",
    "            else:\n",
    "                state_prime = self.gameplay.BOARD.copy() #gamestate\n",
    "            reward = self.gameplay.Get_Score(self.player,state_prime)\n",
    "\n",
    "            if turn % 2 ==0 and self.player==1:\n",
    "                memory.append((state,action,reward,state_prime,status)) #memorize this\n",
    "            if turn % 2 ==1 and self.player==2:\n",
    "                memory.append((state,action,reward,state_prime,status)) #memorize this\n",
    "            turn +=1\n",
    "        return memory    \n",
    "    \n",
    "\n",
    "    #function to hold previous states/actions/rewards/nextstates/status\n",
    "    def memorize(self,state,action,reward,state_prime,done):\n",
    "        if len(self.memory)>=self.memory_size: #if memory is full remove random element before adding new one\n",
    "            self.memory.pop(random.randrange(len(self.memory)))  \n",
    "        self.memory.append((state,action,reward,state_prime,done)) #append memory\n",
    "\n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)\n",
    "    \n",
    "\n",
    "    #predict values of state using model\n",
    "    #I hate shapes/shaping with numpy/keras; can never get it to work so this is a function to reshape it for me\n",
    "    def model_predict(self,state):\n",
    "        state_reshape=np.expand_dims(np.expand_dims(state, axis=3),axis=0)\n",
    "        return self.model.predict(state_reshape)\n",
    "    \n",
    "    #function to take action given state                  \n",
    "    def make_move(self,state): \n",
    "        if np.random.rand() <=self.epsilon:\n",
    "            #explore\n",
    "            #print('explore, epsilon is',self.epsilon)\n",
    "            legal_actions=self.gameplay.Get_Legal_Moves(state)\n",
    "            return random.randrange(len(legal_actions))\n",
    "        else:\n",
    "            #don't explore\n",
    "            qvals = self.model_predict(state)\n",
    "            return np.argmax(qvals)\n",
    "\n",
    "\n",
    "    def replay(self,batch_size):\n",
    "        batch = random.sample(self.memory,self.batch_size)\n",
    "        for el in batch:\n",
    "            if self.player==1: #replace 2's with -1s\n",
    "                el[0][el[0]>1]=-1\n",
    "                el[3][el[3]>1]=-1\n",
    "            if self.player==2: #replace 1s with -1s. replace 2's with 1s\n",
    "                el[0][(el[0] < 2) & (el[0] > 0)] = -1\n",
    "                el[0][el[0]>1]=1                \n",
    "                el[3][(el[3] < 2) & (el[3] > 0)] = -1\n",
    "                el[3][el[3]>1]=1                \n",
    "        \n",
    "        states = []\n",
    "        targets_f = []\n",
    "        for state,action, reward, state_prime,status in batch:\n",
    "            if status != 'Keep Playing!':\n",
    "                target = reward \n",
    "            elif status == 'Keep Playing!':\n",
    "                target = (reward + self.gamma * np.argmax(self.model_predict(state_prime)[0])) #idk why they have first element.\n",
    "\n",
    "            target_f = self.model_predict(state_prime)\n",
    "            targets_f.append(target_f[0])\n",
    "            states.append(state)\n",
    "            \n",
    "        states=np.expand_dims(np.array(states), axis=3) #reshaping to train model\n",
    "        targets_f=np.array(targets_f)\n",
    "\n",
    "        history = self.model.fit(states,targets_f,epochs=1,verbose=0)\n",
    "        loss = history.history['loss'][0]\n",
    "        if self.epsilon <= self.explore_stop:\n",
    "            self.epsilon=self.explore_stop\n",
    "        else:\n",
    "            self.epsilon = self.epsilon-self.decay_rate\n",
    "        #print('epsilon:',self.epsilon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "agent=DeepQLAgent()\n",
    "agent.replay(batch_size)\n",
    "\n",
    "agent2=DeepQLAgent(player=2)\n",
    "agent2.replay(batch_size)\n",
    "\n",
    "\n",
    "gameplay=GamePlay()\n",
    "\n",
    "from Robots import Robots\n",
    "bot=Robots(depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Robots import Robots\n",
    "bot=Robots(depth=1)\n",
    "#bot.MiniMax_bot(state,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon=.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10000\n",
    "episode=0\n",
    "threshold=60\n",
    "\n",
    "batch_size=100\n",
    "turn=0\n",
    "p1wins=0\n",
    "p2wins=0\n",
    "\n",
    "p1prev=0\n",
    "p2prev=0\n",
    "\n",
    "while episode<EPISODES:\n",
    "\n",
    "    #player1\n",
    "    if turn % 2 ==0:\n",
    "        state=gameplay.BOARD.copy() #get state\n",
    "        #print('player1:',state)\n",
    "        action=agent.make_move(state) #get action\n",
    "        legal=gameplay.Get_Legal_Moves(state)\n",
    "        #print('ACTION:',action)\n",
    "        #print(state)\n",
    "        gameplay.Add_Piece(1,action,gameplay.BOARD) #do action\n",
    "        state_prime = gameplay.BOARD.copy() #get next state\n",
    "        reward = gameplay.Get_Score(1,state_prime) #reward for taking action/being in next state\n",
    "        status=gameplay.Check_Goal(gameplay.BOARD)\n",
    "        if action not in legal:\n",
    "            reward = -10000000000000\n",
    "\n",
    "        if status=='Player 1 wins!':\n",
    "            reward==5000\n",
    "        agent.memorize(state,action,reward,state_prime,status)\n",
    "\n",
    "    if turn % 2 ==1:\n",
    "        state=gameplay.BOARD.copy() #state prime\n",
    "        #print('player2:',state)\n",
    "        #action=agent2.make_move(state)\n",
    "        action=bot.MiniMax_bot(state,2)\n",
    "        gameplay.Add_Piece(2,action,gameplay.BOARD)\n",
    "        status=gameplay.Check_Goal(gameplay.BOARD)\n",
    "\n",
    "    #check if it is the end of the episode\n",
    "    if status !='Keep Playing!' or turn>threshold:\n",
    "        if status == 'Player 1 wins!':\n",
    "            p1wins=p1wins+1\n",
    "        if status == 'Player 2 wins!':\n",
    "            p2wins=p2wins+1\n",
    "            \n",
    "        if episode % 500==0:\n",
    "            winrate=(p1wins-p1prev)/(p1wins+p2wins-p1prev-p2prev)\n",
    "            print('EPISODE: ',episode)\n",
    "            print('epsilon:',agent.epsilon)\n",
    "            print('player 1 wins:',p1wins,'player 2 wins:',p2wins, 'winrate:',winrate)\n",
    "            p1prev=p1wins\n",
    "            p2prev=p2wins\n",
    "        if turn % 2 ==1:  #updating reward in our memory if we lost\n",
    "            agent.memory[-1]=(agent.memory[-1][0],agent.memory[-1][1],-5000,agent.memory[-1][3],status) \n",
    "\n",
    "        loss=agent.replay(batch_size) # do the thing\n",
    "        agent.save('weights.h5')\n",
    "        #agent2.load('weights.h5') # load weights of the updated agent for player 2\n",
    "        #agent2.epsilon=agent.epsilon\n",
    "        turn=0\n",
    "        if episode==100:\n",
    "            print('EPISODE:',episode,'epsilon:',agent.epsilon)\n",
    "            print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')\n",
    "\n",
    "        if episode==1000:\n",
    "            print('EPISODE:',episode,'epsilon:',agent.epsilon)\n",
    "            print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')\n",
    "            \n",
    "        if episode==10000:\n",
    "            print('EPISODE:',episode,'epsilon:',agent.epsilon)\n",
    "            print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')\n",
    "        if episode==20000:\n",
    "            print('EPISODE:',episode,'epsilon:',agent.epsilon)\n",
    "            print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')\n",
    "        if episode==50000:\n",
    "            print('EPISODE:',episode,'epsilon:',agent.epsilon)\n",
    "            print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')\n",
    "           \n",
    "        if episode % 100000==0:\n",
    "            print('EPISODE:',episode,'epsilon:',agent.epsilon)\n",
    "            print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')\n",
    "\n",
    "        gameplay.reset()\n",
    "        episode=episode+1\n",
    "        #print('memory:',agent.memory)\n",
    "\n",
    "    turn = turn + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class that creates the connect4 environment\n",
    "class DeepQLAgent():\n",
    "\n",
    "    def __init__(self,player=1,gameplay=GamePlay(),memory=None):\n",
    "        self.player=player\n",
    "        \n",
    "        self.gameplay=gameplay\n",
    "        self.ROWS = self.gameplay.ROWS\n",
    "        self.COLUMNS = self.gameplay.COLUMNS\n",
    "        \n",
    "        self.batch_size = 50\n",
    "        self.lr=.001\n",
    "        self.INPUT_SHAPE=(self.ROWS, self.COLUMNS, 1)\n",
    "        self.action_size =self.COLUMNS\n",
    "        self.gamma=.9\n",
    "        self.epsilon = 1.0 #starting explore probability\n",
    "        self.explore_stop = .01 #stopping explore rate\n",
    "        self.decay_rate = 0.00001 #decay rate # 0.00001\n",
    "\n",
    "        self.memory_size = 5000 # number of experiences to keep\n",
    "        self.memory_start = 50 # starting size of memory bank\n",
    "        self.memory=self._memory_initiate()\n",
    "        \n",
    "        self.model=self._build_model()\n",
    "        self.losses=[]\n",
    "\n",
    "    #build model\n",
    "    def _build_model(self):\n",
    "        model=models.Sequential()\n",
    "        '''model.add(layers.Flatten(input_shape=self.INPUT_SHAPE))\n",
    "        #model.add(Conv2D(64, (3, 3), activation = 'relu')) \n",
    "        model.add(layers.Dense(64, activation = 'relu'))\n",
    "        model.add(layers.Dense(64, activation = 'relu')) \n",
    "        model.add(layers.Dense(32,activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size,activation='linear'))'''\n",
    "\n",
    "        model.add(layers.Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=self.INPUT_SHAPE))\n",
    "\n",
    "        #model.add(Conv2D(64, (3, 3), activation = 'relu')) \n",
    "        model.add(layers.MaxPooling2D(pool_size = (2, 2))) \n",
    "        model.add(layers.Dropout(0.25)) \n",
    "        model.add(layers.Flatten()) \n",
    "        model.add(layers.Dense(64, activation = 'relu')) \n",
    "        model.add(layers.Dense(32,activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size,activation='linear'))\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "    def _memory_initiate(self):\n",
    "        memory=[]\n",
    "        turn = 0\n",
    "        #player=1\n",
    "        status = None\n",
    "        while len(memory)<self.memory_start:\n",
    "            if status != 'Keep Playing!': #if we are in a terminal state restart game\n",
    "                self.gameplay.reset()\n",
    "                turn = 0\n",
    "\n",
    "            state = self.gameplay.BOARD.copy()\n",
    "            actions = self.gameplay.Get_Legal_Moves(self.gameplay.BOARD)\n",
    "            action = np.random.choice(actions,1)[0]\n",
    "            #take random actions. record their states,actions, rewards, next states, and status\n",
    "\n",
    "            #player1\n",
    "            if turn % 2 ==0:\n",
    "                self.gameplay.Add_Piece(1,action,self.gameplay.BOARD)\n",
    "                #turn +=1\n",
    "\n",
    "            #player2\n",
    "            elif turn % 2 ==1:\n",
    "                self.gameplay.Add_Piece(2,action,self.gameplay.BOARD)\n",
    "                #turn +=1\n",
    "\n",
    "\n",
    "            status = self.gameplay.Check_Goal(self.gameplay.BOARD)\n",
    "            if status !='Keep Playing!':\n",
    "                state_prime = np.zeros((self.gameplay.ROWS,self.gameplay.COLUMNS))\n",
    "                state_prime[state_prime < 1] = self.player\n",
    "            else:\n",
    "                state_prime = self.gameplay.BOARD.copy() #gamestate\n",
    "            reward = self.gameplay.Get_Score(self.player,state_prime)\n",
    "\n",
    "            if turn % 2 ==0 and self.player==1:\n",
    "                memory.append((state,action,reward,state_prime,status)) #memorize this\n",
    "            if turn % 2 ==1 and self.player==2:\n",
    "                memory.append((state,action,reward,state_prime,status)) #memorize this\n",
    "            turn +=1\n",
    "        return memory    \n",
    "    \n",
    "\n",
    "    #function to hold previous states/actions/rewards/nextstates/status\n",
    "    def memorize(self,state,action,reward,state_prime,done):\n",
    "        if len(self.memory)>=self.memory_size: #if memory is full remove random element before adding new one\n",
    "            self.memory.pop(random.randrange(len(self.memory)))  \n",
    "        self.memory.append((state,action,reward,state_prime,done)) #append memory\n",
    "\n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)\n",
    "    \n",
    "\n",
    "    #predict values of state using model\n",
    "    #I hate shapes/shaping with numpy/keras; can never get it to work so this is a function to reshape it for me\n",
    "    def model_predict(self,state):\n",
    "        state_reshape=np.expand_dims(np.expand_dims(state, axis=3),axis=0)\n",
    "        return self.model.predict(state_reshape)\n",
    "    \n",
    "    #function to take action given state                  \n",
    "    def make_move(self,state): \n",
    "        if np.random.rand() <=self.epsilon:\n",
    "            #explore\n",
    "            #print('explore, epsilon is',self.epsilon)\n",
    "            legal_actions=self.gameplay.Get_Legal_Moves(state)\n",
    "            return random.randrange(len(legal_actions))\n",
    "        else:\n",
    "            #don't explore\n",
    "            qvals = self.model_predict(state)\n",
    "            return np.argmax(qvals)\n",
    "\n",
    "\n",
    "    def replay(self,batch_size):\n",
    "        batch = random.sample(self.memory,self.batch_size)\n",
    "        for el in batch:\n",
    "            if self.player==1: #replace 2's with -1s\n",
    "                el[0][el[0]>1]=-1\n",
    "                el[3][el[3]>1]=-1\n",
    "            if self.player==2: #replace 1s with -1s. replace 2's with 1s\n",
    "                el[0][(el[0] < 2) & (el[0] > 0)] = -1\n",
    "                el[0][el[0]>1]=1                \n",
    "                el[3][(el[3] < 2) & (el[3] > 0)] = -1\n",
    "                el[3][el[3]>1]=1                \n",
    "        \n",
    "        states = []\n",
    "        targets_f = []\n",
    "        for state,action, reward, state_prime,status in batch:\n",
    "            if status != 'Keep Playing!':\n",
    "                target = reward \n",
    "            elif status == 'Keep Playing!':\n",
    "                if action not in gameplay.Get_Legal_Moves(state): #if not a legal move make target the negative reward\n",
    "                    target = reward\n",
    "                else:\n",
    "                    target = (reward + self.gamma * np.argmax(self.model_predict(state_prime)[0])) #idk why they have first element.\n",
    "\n",
    "            target_f = self.model_predict(state_prime)\n",
    "            target_f[0][action]=target\n",
    "            targets_f.append(target_f[0])\n",
    "            states.append(state)\n",
    "            \n",
    "        states=np.expand_dims(np.array(states), axis=3) #reshaping to train model\n",
    "        targets_f=np.array(targets_f)\n",
    "\n",
    "        history = self.model.fit(states,targets_f,epochs=1,verbose=0)\n",
    "        loss = history.history['loss'][0]\n",
    "        self.losses.append(loss)\n",
    "        if self.epsilon <= self.explore_stop:\n",
    "            self.epsilon=self.explore_stop\n",
    "        else:\n",
    "            self.epsilon = self.epsilon-self.decay_rate\n",
    "        #print('epsilon:',self.epsilon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wamsa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:111: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "C:\\Users\\wamsa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:111: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"
     ]
    }
   ],
   "source": [
    "gameplay=GamePlay()\n",
    "batch_size=50\n",
    "agent=DeepQLAgent()\n",
    "agent.replay(batch_size)\n",
    "\n",
    "agent2=DeepQLAgent(player=2)\n",
    "agent2.replay(batch_size)\n",
    "\n",
    "\n",
    "from Robots import Robots\n",
    "bot=Robots(depth=1)\n",
    "#bot.MiniMaxAlphaBeta_bot(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:  0\n",
      "Avg loss last 1000: 162816.11 epsilon: 0.99999\n",
      "player 1 wins: 0 player 2 wins: 1 winrate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wamsa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:111: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 0 epsilon: 0.9999800000000001\n",
      "2020-12-01 17:09:21\n",
      "EPISODE: 100 epsilon: 0.9989800000000046\n",
      "2020-12-01 17:09:42\n",
      "EPISODE:  1000\n",
      "Avg loss last 1000: 125656.12 epsilon: 0.9899900000000456\n",
      "player 1 wins: 26 player 2 wins: 975 winrate: 0.03\n",
      "EPISODE: 1000 epsilon: 0.9899800000000456\n",
      "2020-12-01 17:12:59\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-75c7529aea16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mp2prev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp2wins\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# do the thing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weights.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a8d49decbd09>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    148\u001b[0m                     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_prime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#idk why they have first element.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[0mtarget_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_prime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mtargets_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a8d49decbd09>\u001b[0m in \u001b[0;36mmodel_predict\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mstate_reshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_reshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;31m#function to take action given state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#secondgo\n",
    "EPISODES = 200000\n",
    "episode=0\n",
    "threshold=60\n",
    "\n",
    "batch_size=50\n",
    "turn=0\n",
    "p1wins=0\n",
    "p2wins=0\n",
    "\n",
    "p1prev=0\n",
    "p2prev=0\n",
    "\n",
    "#agent.epsilon=.5 .6\n",
    "#agent.epsilon=.2\n",
    "\n",
    "while episode<EPISODES:\n",
    "\n",
    "    #player1\n",
    "    if turn % 2 ==0:\n",
    "        state=gameplay.BOARD.copy() #get state\n",
    "        action=agent.make_move(state) #get action\n",
    "        gameplay.Add_Piece(1,action,gameplay.BOARD) #do action\n",
    "        status=gameplay.Check_Goal(gameplay.BOARD)\n",
    "\n",
    "    if turn % 2 ==1:\n",
    "        #action2=bot.Rando_bot(state)\n",
    "        #action2=agent2.make_move(state)\n",
    "        action2=bot.MiniMaxAlphaBeta_bot(state,2)\n",
    "        gameplay.Add_Piece(2,action2,gameplay.BOARD)\n",
    "        state_prime = gameplay.BOARD.copy()\n",
    "        reward = 0#gameplay.Get_Score(1,state_prime)\n",
    "        status=gameplay.Check_Goal(gameplay.BOARD)\n",
    "\n",
    "    if turn % 2 ==0:\n",
    "        if status == 'Player 1 wins!':\n",
    "            reward=5000\n",
    "            state_prime = gameplay.BOARD.copy()\n",
    "            agent.memorize(state,action,reward,state_prime,status)\n",
    "        if status =='Player 2 wins!':\n",
    "            reward=-5000\n",
    "            agent.memorize(state,action,reward,state_prime,status)\n",
    "        elif turn==0:\n",
    "            pass\n",
    "        else:\n",
    "            if action not in gameplay.Get_Legal_Moves(gameplay.BOARD):\n",
    "                reward=-5000\n",
    "            agent.memorize(state,action,reward,state_prime,status)\n",
    "\n",
    "\n",
    "    #check if it is the end of the episode\n",
    "    if status !='Keep Playing!' or turn>threshold:\n",
    "        if status == 'Player 1 wins!':\n",
    "            p1wins=p1wins+1\n",
    "        if status == 'Player 2 wins!':\n",
    "            p2wins=p2wins+1\n",
    "\n",
    "\n",
    "        if episode % 1000==0:\n",
    "            print('EPISODE: ',episode)\n",
    "            print('Avg loss last 1000:',round(sum(agent.losses)/len(agent.losses)+.001,2),'epsilon:',agent.epsilon)\n",
    "            agent.losses=[]\n",
    "            winrate=round((p1wins-p1prev)/(p1wins+p2wins-p1prev-p2prev+.001),2)\n",
    "            print('player 1 wins:',p1wins,'player 2 wins:',p2wins, 'winrate:',winrate)\n",
    "            p1prev=p1wins\n",
    "            p2prev=p2wins\n",
    "\n",
    "        loss=agent.replay(batch_size) # do the thing\n",
    "        \n",
    "        agent.save('weights.h5')\n",
    "        agent2.load('weights.h5') # load weights of the updated agent for player 2\n",
    "        agent2.epsilon=agent.epsilon\n",
    "        turn=0\n",
    "        if episode==100:\n",
    "            print('EPISODE:',episode,'epsilon:',agent.epsilon)\n",
    "            print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')\n",
    "\n",
    "        if episode==1000:\n",
    "            print('EPISODE:',episode,'epsilon:',agent.epsilon)\n",
    "            print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')\n",
    "            \n",
    "        if episode==10000:\n",
    "            print('EPISODE:',episode,'epsilon:',agent.epsilon)\n",
    "            print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')\n",
    "        if episode==20000:\n",
    "            print('EPISODE:',episode,'epsilon:',agent.epsilon)\n",
    "            print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')\n",
    "        if episode==50000:\n",
    "            print('EPISODE:',episode,'epsilon:',agent.epsilon)\n",
    "            print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')\n",
    "           \n",
    "        if episode % 100000==0:\n",
    "            print('EPISODE:',episode,'epsilon:',agent.epsilon)\n",
    "            print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')\n",
    "\n",
    "        gameplay.reset()\n",
    "        episode=episode+1\n",
    "        #print('memory:',agent.memory)\n",
    "\n",
    "    turn = turn + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for el in batch:\n",
    "            if self.player==1: #replace 2's with -1s\n",
    "                el[0][el[0]>1]=-1\n",
    "                el[3][el[3]>1]=-1\n",
    "            if self.player==2: #replace 1s with -1s. replace 2's with 1s\n",
    "                el[0][(el[0] < 2) & (el[0] > 0)] = -1\n",
    "                el[0][el[0]>1]=1                \n",
    "                el[3][(el[3] < 2) & (el[3] > 0)] = -1\n",
    "                el[3][el[3]>1]=1                \n",
    "        \n",
    "        states = []\n",
    "        targets_f = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "targets_f = []\n",
    "batch = random.sample(agent.memory,agent.batch_size)\n",
    "for state,action, reward, state_prime,status in batch:\n",
    "    if status != 'Keep Playing!':\n",
    "        target = reward\n",
    "    elif status == 'Keep Playing!':\n",
    "        if action not in gameplay.Get_Legal_Moves(state): #if not a legal move make target the negative reward\n",
    "            target = reward\n",
    "        else:\n",
    "             target = (reward + agent.gamma * np.argmax(agent.model_predict(state_prime))) #idk why they have first element.\n",
    "\n",
    "    target_f = agent.model_predict(state_prime)\n",
    "    print(target_f[0][action])\n",
    "    targets_f.append(target_f[0][action])\n",
    "    states.append(state)\n",
    "    #print('STATE:',state)\n",
    "    #print('ACTION:',action)\n",
    "    #print('target:',target)\n",
    "    #print('reward:',reward)\n",
    "\n",
    "            \n",
    "states=np.expand_dims(np.array(states), axis=3) #reshaping to train model\n",
    "targets_f=np.array(targets_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mem in agent.memory:\n",
    "    #print(mem[0])\n",
    "    print(np.argmax(agent.model_predict(mem[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "keras.models.save_model(agent.model,'mymodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory[-1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameplay.BOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if status != 'Keep Playing!': #if we are in a terminal state restart game\n",
    "    self.gameplay.reset()\n",
    "    turn = 0\n",
    "\n",
    "state = self.gameplay.BOARD.copy()\n",
    "actions = self.gameplay.Get_Legal_Moves(self.gameplay.BOARD)\n",
    "action = np.random.choice(actions,1)[0]\n",
    "#take random actions. record their states,actions, rewards, next states, and status\n",
    "\n",
    "    #player1\n",
    "if turn % 2 ==0:\n",
    "    self.gameplay.Add_Piece(1,action,self.gameplay.BOARD)\n",
    "        #turn +=1\n",
    "\n",
    "    #player2\n",
    "elif turn % 2 ==1:\n",
    "    self.gameplay.Add_Piece(2,action,self.gameplay.BOARD)\n",
    "        #turn +=1\n",
    "\n",
    "\n",
    "status = self.gameplay.Check_Goal(self.gameplay.BOARD)\n",
    "if status !='Keep Playing!':\n",
    "    state_prime = np.zeros((self.gameplay.ROWS,self.gameplay.COLUMNS))\n",
    "    state_prime[state_prime < 1] = 1\n",
    "else:\n",
    "    state_prime = self.gameplay.BOARD.copy() #gamestate\n",
    "reward = self.gameplay.get_reward(self.player,state)\n",
    "\n",
    "if turn % 2 ==0 and self.player==1:\n",
    "    memory.append((state,action,reward,state_prime,status)) #memorize this\n",
    "if turn % 2 ==1 and self.player==2:\n",
    "    memory.append((state,action,reward,state_prime,status)) #memorize this\n",
    "turn +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keras.models.save_model(\n",
    "agent1.model,'mymodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model and using it for predictions\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "\n",
    "keras.models.save_model(\n",
    "agent.model,'mymodel.h5')\n",
    "\n",
    "loaded_model = load_model('mymodel.h5')\n",
    "state=np.expand_dims(np.expand_dims(gameplay.BOARD, axis=3),axis=0)\n",
    "loaded_model.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=np.expand_dims(np.expand_dims(gameplay.BOARD, axis=3),axis=0)\n",
    "loaded_model.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model=models.Sequential()\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu', input_shape=INPUT_SHAPE))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(layers.Dense(action_size,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
