{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Connect4 import Connect4\n",
    "from Robots import Robots\n",
    "from GamePlay import GamePlay\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras import losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class that creates the connect4 environment\n",
    "class DeepQLAgent():\n",
    "\n",
    "    def __init__(self,player=1,gameplay=GamePlay(),memory=None):\n",
    "        self.player=player\n",
    "        \n",
    "        self.gameplay=gameplay\n",
    "        self.ROWS = self.gameplay.ROWS\n",
    "        self.COLUMNS = self.gameplay.COLUMNS\n",
    "        \n",
    "        self.batch_size = 200\n",
    "        self.lr=.01\n",
    "        self.INPUT_SHAPE=(self.ROWS, self.COLUMNS, 1)\n",
    "        self.action_size =self.COLUMNS\n",
    "        self.gamma=.9\n",
    "        self.epsilon = .6 #starting explore probability\n",
    "        self.explore_stop = .01 #stopping explore rate\n",
    "        self.decay_rate = 0.99995\n",
    "        \n",
    "        self.memory_size=10000\n",
    "        self.model=self._build_model()\n",
    "        self.model2=self._build_model()\n",
    "        self.c=0\n",
    "        \n",
    "        self.losses=[]\n",
    "        self.avgrewards=[]\n",
    "        self.memory=[]\n",
    "\n",
    "    #build model\n",
    "    def _build_model(self):\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Conv2D(64, (4,4), input_shape=self.INPUT_SHAPE))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        \n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(64,activation='relu'))\n",
    "        model.add(layers.Dense(64,activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size,activation='linear'))\n",
    "        \n",
    "        optimizer = keras.optimizers.Adam(lr=self.lr)\n",
    "        l=losses.MeanSquaredError()\n",
    "        model.compile(loss=l, optimizer=optimizer)\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "    #function to hold previous states/actions/rewards/nextstates/status\n",
    "    def memorize(self,state,action,reward,state_prime,done):\n",
    "        if len(self.memory)>=self.memory_size: #if memory is full remove earliest element before adding new one\n",
    "            #self.memory.pop(0)\n",
    "            self.memory.pop(random.randrange(len(self.memory)))\n",
    "        self.memory.append((state,action,reward,state_prime,done)) #append memory\n",
    "\n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "\n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)\n",
    "    \n",
    "\n",
    "    #predict values of state using model\n",
    "    def model_predict(self,state):\n",
    "        state_reshape=np.expand_dims(np.expand_dims(state, axis=3),axis=0)\n",
    "        return self.model.predict(state_reshape)\n",
    "\n",
    "    #predict values of state using model2\n",
    "    def model2_predict(self,state):\n",
    "        state_reshape=np.expand_dims(np.expand_dims(state, axis=3),axis=0)\n",
    "        return self.model2.predict(state_reshape)\n",
    "    \n",
    "    \n",
    "    #function to take action given state                  \n",
    "    def make_move(self,state,p): \n",
    "        if p==2:\n",
    "            state=self.p2_swap(state)\n",
    "        if np.random.rand() <=self.epsilon:\n",
    "            #explore\n",
    "            legal_actions=self.gameplay.Get_Legal_Moves(state)\n",
    "            return random.randrange(len(legal_actions))\n",
    "        else:\n",
    "            #don't explore\n",
    "            qvals = self.model_predict(state)\n",
    "            return np.argmax(qvals)\n",
    "\n",
    "    #helper function to swap 2s and 1s if player 2\n",
    "    def p2_swap(self,b):\n",
    "        c=[]\n",
    "        for row in b:\n",
    "            rowvec=[]\n",
    "            for el in row:\n",
    "                if el==1:\n",
    "                    rowvec.append(2)\n",
    "                if el==2:\n",
    "                    rowvec.append(1)\n",
    "                if el==0:\n",
    "                    rowvec.append(0)\n",
    "            c.append(rowvec)\n",
    "\n",
    "        return np.array(c)\n",
    "        \n",
    "        \n",
    "    def learn(self,batch_size):\n",
    "        if len(self.memory)<batch_size:\n",
    "            return\n",
    "        if self.c>=50:\n",
    "            self.c=0\n",
    "            self.model2.set_weights(self.model.get_weights()) #periodically update model 2 weights\n",
    "        \n",
    "        batch = random.sample(self.memory,self.batch_size)\n",
    "        X = []\n",
    "        y = []\n",
    "        ##############################################################################################\n",
    "        ### iterate through batch:\n",
    "        ##############################################################################################\n",
    "        for state, action, reward, state_prime, status in batch:\n",
    "            #swap 2s and 1s for training against itself\n",
    "            if self.player==2:\n",
    "                state=self.p2_swap(state)\n",
    "            \n",
    "            qs=self.model_predict(state)[0] #get qs\n",
    "            if status!='Keep Playing!': #if terminal, q[s,a] is reward\n",
    "                target=reward\n",
    "            else:\n",
    "                #otherwise, q[s,a] is the immediate reward plus discounted future reward\n",
    "                #future reward is calculated using model2 on stateprime, and getting argmax\n",
    "                target = reward + self.gamma*np.argmax(self.model2_predict(state_prime))\n",
    "            \n",
    "            qs[action]=target #updating this state's qvalues for the action taken\n",
    "            \n",
    "            #while we are here, why not update the other q values if their action leads to a winning state:\n",
    "            for a in range(len(qs)):\n",
    "                b=state.copy() #pretend board\n",
    "                s=gameplay.Add_Piece(1,a,b) #pretend add piece\n",
    "                if gameplay.Check_Goal(s)!='Keep Playing!': #see if pretend status is terminal\n",
    "                    qs[a]=gameplay.get_reward(self.player,s) #if it is, update q[s,a] with the reward\n",
    "                    continue\n",
    "                b2=state.copy()\n",
    "                s2=gameplay.Add_Piece(2,a,b2)\n",
    "                if gameplay.Check_Goal(s)!='Keep Playing!': #see if pretend status is terminal\n",
    "                    qs[a]=20 #make the moves that counter wins more favorable\n",
    "\n",
    "            X.append(state) #append for training\n",
    "            y.append(qs) #new and imporved q values,ready for training\n",
    "        ##############################################################################################\n",
    "        ##############################################################################################         \n",
    "        \n",
    "        #reshape data elements before retraining model:\n",
    "        X=np.expand_dims(np.array(X), axis=3)  \n",
    "        y=np.array(y)\n",
    "        \n",
    "        #updating model:\n",
    "        history = self.model.fit(X,y, verbose=0)\n",
    "        self.c = self.c+1 #add 1 to constant\n",
    "        loss = history.history['loss'][0]\n",
    "        self.losses.append(loss)\n",
    "        if self.epsilon <= self.explore_stop:\n",
    "            self.epsilon=self.explore_stop\n",
    "        else:\n",
    "            self.epsilon = self.epsilon*self.decay_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randrange(3)   \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyprint(mem,q=False):\n",
    "    print('-------------------------------------------------')\n",
    "    for row in mem[0]:\n",
    "        print(row)\n",
    "    if q==True:\n",
    "        qs=agent.model_predict(mem[0])[0]\n",
    "        for i in range(len(qs)):\n",
    "            qs[i]=round(qs[i],4)\n",
    "        print('Q values:')\n",
    "        print(qs)\n",
    "\n",
    "    print('-------------------------------------------------')\n",
    "    print('ACTION:',mem[1],'REWARD:',mem[2],'STATUS:',mem[4])\n",
    "    print('-------------------------------------------------')\n",
    "    for row in mem[3]:\n",
    "        print(row)\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "#print(prettyprint(memory[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function\n",
    "def get_status_reward(state,state_prime,action,p):\n",
    "    if p==1 and action not in gameplay.Get_Legal_Moves(state):\n",
    "        return 'Illegal Move',-15\n",
    "    else:\n",
    "        return gameplay.Check_Goal(state_prime), gameplay.get_reward(1,state_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=DeepQLAgent()\n",
    "agent.load('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gameplay=GamePlay()\n",
    "#agent=DeepQLAgent()\n",
    "\n",
    "from Robots import Robots\n",
    "bot=Robots(depth=2)\n",
    "\n",
    "\n",
    "EPISODES = 50000\n",
    "episode=0\n",
    "wins=0\n",
    "rewardsum=0\n",
    "\n",
    "memory=[]\n",
    "while episode<EPISODES:\n",
    "    \n",
    "    state=gameplay.BOARD.copy()#get state\n",
    "    #player 1 moves\n",
    "    if gameplay.Check_Goal(gameplay.BOARD)=='Keep Playing!': #only do an action if it is not terminal\n",
    "        action=agent.make_move(state,1) #get action\n",
    "        gameplay.Add_Piece(1,action,gameplay.BOARD) #do action\n",
    "        state_prime = gameplay.BOARD.copy() #get state prime\n",
    "        \n",
    "        #update status, #get reward\n",
    "        status,reward=get_status_reward(state,state_prime,action,1)\n",
    "        \n",
    "################################################################\n",
    "    #player 2\n",
    "    if status=='Keep Playing!':  #only do an action if it is not terminal\n",
    "        \n",
    "        #three actions generated in a list (agent,rando,or minimax). randomly pick one\n",
    "        l=[agent.make_move(state_prime,2),bot.Rando_bot(state_prime),bot.MiniMaxAlphaBeta_bot(state_prime,2)]\n",
    "        action2=l[random.randrange(3)]\n",
    "        \n",
    "        gameplay.Add_Piece(2,action2,gameplay.BOARD) #do action\n",
    "        state_prime = gameplay.BOARD.copy()\n",
    "        status,reward=get_status_reward(state,state_prime,action,2)\n",
    "    \n",
    "    #memorize this\n",
    "    agent.memorize(state,action,reward,state_prime,status)\n",
    "\n",
    "    #track sum of rewards:\n",
    "    rewardsum=rewardsum+reward\n",
    "    \n",
    "    if status!='Keep Playing!': #reset board\n",
    "        #track who won\n",
    "        if status=='Player 1 wins!':\n",
    "            wins=wins+1\n",
    "        gameplay.reset()\n",
    "        episode=episode+1\n",
    "        #LEARNNNNNNNNNN\n",
    "        agent.learn(agent.batch_size)\n",
    "        \n",
    "\n",
    "    if episode % 100 ==0 and status!='Keep Playing!':\n",
    "        print('--------------------------------------------------------')\n",
    "        print('SUMMARY:')\n",
    "        print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        print('EPISODE: ',episode)\n",
    "        print('SUM OF REWARDS:',rewardsum)\n",
    "        print('Win Rate:',wins/100)\n",
    "        print('Avg loss last 100:',round(sum(agent.losses)/(len(agent.losses)+.001),2),'epsilon:',round(agent.epsilon,3))\n",
    "        wins=0\n",
    "        rewardsum=0\n",
    "    if episode % 1000==0 and status !='Keep Playing!':\n",
    "        keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')\n",
    "        \n",
    "        \n",
    "#for mem in memory:\n",
    "#    print(prettyprint(mem,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(agent.memory)):\n",
    "    #if agent.memory[i][4]!='Keep Playing!':\n",
    "    prettyprint(agent.memory[i],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(agent.model,'mymodel_'+str(episode)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
